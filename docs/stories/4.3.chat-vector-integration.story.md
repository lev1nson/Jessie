# Story 4.3: Подключение chat API к vector search

## Status
Ready for Development

## Story
**As a** пользователь,
**I want** получать реальные ответы на основе моих писем через chat interface,
**so that** я мог анализировать свой email архив через разговор с Jessie.

## Acceptance Criteria
1. Chat API использует real vector search вместо mock data
2. Search results основаны на user's emails с proper data isolation
3. LLM responses генерируются на основе найденных email content
4. Chat interface показывает source emails для каждого ответа
5. Performance: ответы генерируются в течение 3 секунд

## 🚀 **QUICK START**
Before starting, review: **docs/DEVELOPER_QUICK_REFERENCE.md** for essential commands and checklists.

## Tasks / Subtasks

### Task 1: Replace Mock Data с Real Vector Search (AC: 1)
- [ ] Модифицировать `/api/chat/messages/route.ts`
- [ ] **CRITICAL**: Remove ALL hardcoded mock responses from chat API
- [ ] **CRITICAL**: Verify no `return mockChatResponse()` calls remain
- [ ] **CRITICAL**: Ensure vector search called for EVERY chat message
- [ ] Интегрировать VectorRepository для semantic search
- [ ] Заменить mock responses на real vector search results
- [ ] Add user authentication validation для API calls
- [ ] Test with user who has NO emails (empty state handling)

### Task 2: User Data Isolation (AC: 2)
- [ ] Implement user-specific vector search queries
- [ ] Add user_id filtering в VectorRepository queries
- [ ] Ensure no data leakage между users
- [ ] Test data isolation в multi-user scenarios

### Task 3: LLM Response Generation (AC: 3)
- [ ] Integrate OpenAI/LLM service для response generation
- [ ] Create prompts для generating responses from email content
- [ ] Implement context window management для large results
- [ ] Add fallback responses для empty search results

### Task 4: Source Email Attribution (AC: 4)
- [ ] Return source email IDs с each response
- [ ] Modify ChatMessages component для displaying sources
- [ ] Add click handlers для viewing source emails
- [ ] Implement email preview functionality

### Task 5: Performance Optimization (AC: 5)
- [ ] Optimize vector search query performance
- [ ] Implement response caching для common queries
- [ ] Add parallel processing где возможно
- [ ] Set reasonable timeouts для all API calls

### Task 6: Enhanced Chat Experience (AC: 1-5)
- [ ] Add typing indicators во время search/generation
- [ ] Implement streaming responses для long answers
- [ ] Add error handling для all failure scenarios
- [ ] Create helpful error messages для users

## Dev Notes

### Current Implementation Status
- ✅ Chat API endpoints exist (Story 2.4) но возвращают mock data
- ✅ VectorRepository полностью реализован (Story 1.5)
- ✅ ChatInterface готов для real data
- ✅ LLM services (OpenAI client) готовы
- ❌ Integration между chat API и vector search отсутствует
- ❌ Source attribution не реализован

### Integration Architecture
```typescript
// Modified /api/chat/messages/route.ts
export async function POST(request: Request) {
  const { chatId, content } = await request.json()
  const user = await getCurrentUser()
  
  // 1. Perform vector search
  const searchResults = await vectorRepository.findSimilar(
    content, 
    { userId: user.id, limit: 5, threshold: 0.7 }
  )
  
  // 2. Generate LLM response
  const response = await llmService.generateResponse(
    content, 
    searchResults
  )
  
  // 3. Save and return message with sources
  const message = await messageRepository.save({
    chatId,
    role: 'assistant',
    content: response.text,
    sourceEmailIds: searchResults.map(r => r.emailId)
  })
  
  return { message, sources: searchResults }
}
```

### File Locations
```
apps/web/app/api/chat/messages/route.ts              # Main integration point
apps/web/lib/repositories/vectorRepository.ts        # Existing vector search
apps/web/lib/llm/openaiClient.ts                    # Existing LLM client
apps/web/components/features/chat/ChatMessages.tsx   # UI updates needed
apps/web/components/features/chat/MessageItem.tsx    # Source display
```

### Required API Modifications

#### 1. Vector Search Integration
```typescript
// Replace mock data with real search
const searchResults = await vectorRepository.findSimilar(query, {
  userId: user.id,
  limit: 5,
  threshold: 0.7,
  includeMetadata: true
})
```

#### 2. LLM Response Generation
```typescript
async function generateContextualResponse(
  query: string, 
  emailContext: SearchResult[]
): Promise<string> {
  const prompt = `
    Based on the following emails, answer the user's question: "${query}"
    
    Email Context:
    ${emailContext.map(email => `
      From: ${email.metadata.from}
      Subject: ${email.metadata.subject}
      Content: ${email.content}
    `).join('\n\n')}
    
    Answer:
  `
  
  return await openaiClient.generateResponse(prompt)
}
```

### Frontend Updates Required

#### 1. Source Email Display
```typescript
// In MessageItem.tsx, add source indicators
interface MessageWithSources {
  content: string
  sources?: EmailSource[]
}

function SourceIndicator({ sources }: { sources: EmailSource[] }) {
  return (
    <div className="mt-2 text-sm text-gray-600">
      Sources: {sources.length} emails
      {sources.map(source => (
        <button key={source.id} onClick={() => openEmailPreview(source)}>
          {source.subject}
        </button>
      ))}
    </div>
  )
}
```

#### 2. Loading States
```typescript
// Enhanced loading states for vector search
const [isSearching, setIsSearching] = useState(false)
const [isGenerating, setIsGenerating] = useState(false)

// Show "Searching emails..." then "Generating response..."
```

### Success Metrics
- [ ] 100% real data responses (no mock data)
- [ ] <3 seconds average response time
- [ ] >90% relevant search results based on user feedback
- [ ] Source attribution for 100% of responses
- [ ] No data leakage между users

### API Response Format
```typescript
interface ChatResponse {
  message: {
    id: string
    content: string
    role: 'assistant'
    createdAt: string
    sourceEmailIds: string[]
  }
  sources: {
    id: string
    subject: string
    from: string
    snippet: string
    relevanceScore: number
  }[]
}
```

### Performance Targets
- Vector search: <1 second for 1000+ emails
- LLM response generation: <2 seconds
- Total API response: <3 seconds
- Frontend rendering: <500ms after API response

### Error Handling Scenarios
1. **No Search Results**: "I couldn't find relevant emails for your question"
2. **Vector Search Timeout**: "Search is taking longer than expected, please try again"
3. **LLM Generation Error**: "I'm having trouble generating a response right now"
4. **Authentication Error**: Redirect to login

### 🧪 **TESTING COMMANDS**

#### Test Mock Removal (CRITICAL)
```bash
# 1. Verify no mock responses remain
grep -r "mockChatResponse\|mock.*response" apps/web/app/api/chat/
# Should return: no results

# 2. Test real vector search integration
curl -X POST "https://your-app.vercel.app/api/chat/messages" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-session" \
  -d '{"chatId":"test","content":"Show me emails about project updates"}'

# 3. Verify response contains real data
# Response should include:
# - sourceEmailIds: [...] (array with email IDs, not empty)
# - message.content: (real LLM-generated response, not mock text)
```

#### Test Vector Search Functionality
```bash
# 4. Test with user who has emails
# Chat response should return relevant emails based on query

# 5. Test with user who has NO emails  
# Should return: "I couldn't find relevant emails for your question"

# 6. Test data isolation
# User A should only see responses based on User A's emails
```

#### Test Performance
```bash
# 7. Test response time
time curl -X POST "https://your-app.vercel.app/api/chat/messages" \
  -H "Content-Type: application/json" \
  -d '{"chatId":"test","content":"test query"}'
# Should complete in <3 seconds
```

### Testing Strategy
1. **Unit Tests**: Mock VectorRepository в chat API tests
2. **Integration Tests**: Real search с test data
3. **User Tests**: Multi-user data isolation verification  
4. **Performance Tests**: Load testing с realistic query volumes
5. **E2E Tests**: Complete user journey from login to chat response
6. **Mock Removal Tests**: Verify no hardcoded responses remain

### Security Considerations
- User authentication на all API endpoints
- User data isolation в vector queries
- Input sanitization для user queries
- Rate limiting для prevent abuse
- No exposure sensitive email content в logs

### Monitoring Requirements
- Query response times
- Search result relevance scores
- LLM API usage и costs
- User satisfaction metrics
- Error rates by category

## Dependencies
- **Requires**: Story 4.2 (Vectorization integration) для иметь vectorized data
- **Requires**: OpenAI API для response generation
- **Requires**: Authenticated users с email data
- **Enables**: Full MVP functionality

## Risks
- **HIGH**: Poor search relevance affects user experience
- **MEDIUM**: LLM costs may scale quickly с usage
- **MEDIUM**: Performance issues с large vector databases
- **LOW**: User confusion без proper source attribution

### ✅ **COMPLETION VERIFICATION**

**Story 4.3 is DONE when:**
```bash
# 1. No mock responses remain
grep -r "mock" apps/web/app/api/chat/ | wc -l  # Should return: 0

# 2. Chat uses real vector search
curl -X POST "https://your-app.vercel.app/api/chat/messages" \
  -d '{"chatId":"test","content":"test"}' | jq '.sourceEmailIds | length'
# Should return: number > 0 (if user has emails)

# 3. Response time acceptable
time curl -X POST "https://your-app.vercel.app/api/chat/messages" \
  -d '{"chatId":"test","content":"test"}'
# Should complete in <3 seconds

# 4. Data isolation working
# Different users get different search results based on their emails
```

## Change Log
| Date | Version | Description | Author |
| :---- | :---- | :---- | :---- |
| 27.07.2025 | 1.0 | Story creation для Epic 4 integration | Sarah (PO) |
| 27.07.2025 | 1.1 | Added mock removal checklist and testing commands | Bob (SM) |